{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Supervised Classification: Decision Trees, SVM, and Naive Bayes | Assignment Solution\n",
        "\n",
        "This notebook provides the complete solution for the Supervised Classification Assignment (DA-AG-009)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1"
      },
      "source": [
        "## Question 1: What is Information Gain, and how is it used in Decision Trees?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1"
      },
      "source": [
        "**Information Gain (IG)** is a metric used by Decision Tree algorithms (like ID3 and C4.5) to decide which feature to split on at each node in order to build the tree.\n",
        "\n",
        "* **Definition:** It measures the reduction in **Entropy** (or impurity) of a dataset after splitting it based on an attribute. In essence, it tells us how much 'useful' information a particular feature provides.\n",
        "* **Usage in Decision Trees:** At every step, the Decision Tree algorithm calculates the Information Gain for every remaining feature. It selects the feature that yields the **highest Information Gain** to make the split. This ensures that the tree partitions the data most effectively, moving toward purer child nodes and reducing the overall tree depth. The goal is to maximize the purity of the resulting subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2"
      },
      "source": [
        "## Question 2: What is the difference between Gini Impurity and Entropy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2"
      },
      "source": [
        "Both **Gini Impurity** and **Entropy** are metrics used by Decision Trees to measure the impurity or randomness of a node, helping the algorithm decide the best split.\n",
        "\n",
        "| Feature | Gini Impurity (Used by CART) | Entropy (Used by ID3, C4.5) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Definition** | Measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution. | Measures the expected amount of information (or randomness) required to classify a data point in the set. |\n",
        "| **Mathematical Form** | $\\sum_{i=1}^{C} p_i (1 - p_i)$ | $-\\sum_{i=1}^{C} p_i \\log_2(p_i)$ |\n",
        "| **Range** | 0 (pure) to 0.5 (maximum impurity, for 2 classes) | 0 (pure) to 1 (maximum impurity, for 2 classes) |\n",
        "| **Computation** | Involves squaring probabilities, making it computationally faster. | Involves logarithmic calculations, which can be slightly slower. |\n",
        "\n",
        "**Key Difference:** While they often lead to similar tree structures, Gini tends to isolate the most frequent class in its own branch, while Entropy tends to produce slightly more balanced trees. However, due to its simplicity and computational efficiency, **Gini Impurity is often the default choice** in many modern libraries like scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3"
      },
      "source": [
        "## Question 3: What is Pre-Pruning in Decision Trees?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3"
      },
      "source": [
        "**Pre-pruning** is a technique used to **prevent overfitting** in Decision Tree models by stopping the tree construction process *before* it reaches full depth.\n",
        "\n",
        "* **Mechanism (The \"Stop Early\" Approach):** The algorithm applies constraints during the training phase. If a node does not meet a specified criterion, the splitting process for that branch is halted, and the node is made a leaf node.\n",
        "* **Common Criteria:**\n",
        "    1.  **Maximum Depth (`max_depth`):** Limiting the maximum number of levels in the tree.\n",
        "    2.  **Minimum Samples in a Leaf (`min_samples_leaf`):** Requiring a minimum number of samples in any terminal node.\n",
        "    3.  **Minimum Impurity Decrease (`min_impurity_decrease`):** Requiring a split to provide a minimum reduction in impurity (Gini or Entropy).\n",
        "\n",
        "Pre-pruning generally results in a smaller, simpler tree, which helps **reduce variance and improve generalization** to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4"
      },
      "source": [
        "## Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q4_code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Decision Tree Feature Importances (Gini) ---\n",
            "Feature C: 0.6713\n",
            "Feature A: 0.2215\n",
            "Feature D: 0.0575\n",
            "Feature B: 0.0497\n",
            "\n",
            "Test Accuracy: 0.8900\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Generate synthetic data (4 features, 2 informative)\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=4,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    random_state=42\n",
        ")\n",
        "feature_names = ['Feature A', 'Feature B', 'Feature C', 'Feature D']\n",
        "\n",
        "# 2. Split data (optional but good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize and train the Decision Tree Classifier using Gini\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# 4. Print feature importances\n",
        "print(\"--- Decision Tree Feature Importances (Gini) ---\")\n",
        "importances = dt_classifier.feature_importances_\n",
        "for name, importance in sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# Note: Features A and C should show higher importance as they were set as informative\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5"
      },
      "source": [
        "## Question 5: What is a Support Vector Machine (SVM)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5"
      },
      "source": [
        "A **Support Vector Machine (SVM)** is a powerful and versatile supervised machine learning algorithm used for both classification and regression, though it's primarily known for classification.\n",
        "\n",
        "* **Goal:** The main objective of an SVM is to find the optimal **hyperplane** (a decision boundary) that distinctly classifies the data points. This hyperplane maximizes the **margin**—the distance between the hyperplane and the nearest data points of any class.\n",
        "* **Support Vectors:** The data points closest to the hyperplane are called **Support Vectors**. These points are crucial because they alone define the position and orientation of the hyperplane, making the model robust even with large datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6"
      },
      "source": [
        "## Question 6: What is the Kernel Trick in SVM?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6"
      },
      "source": [
        "The **Kernel Trick** is a fundamental technique that allows SVMs to perform effective classification even when the data is **not linearly separable** in its original feature space.\n",
        "\n",
        "* **Problem:** If data points cannot be separated by a straight line (or flat hyperplane), standard linear SVM fails.\n",
        "* **Solution:** The Kernel Trick uses a **kernel function** (e.g., Polynomial, Radial Basis Function/RBF) to implicitly map the data points from the low-dimensional feature space to a **much higher-dimensional feature space** where the data *can* be linearly separated. Crucially, the kernel function does this mapping without ever explicitly calculating the coordinates in the new, higher-dimensional space. It computes the dot product of the mapped points directly in the original space, saving huge amounts of computational cost.\n",
        "* **Benefit:** It allows us to fit non-linear models while keeping the computational simplicity of a linear algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7"
      },
      "source": [
        "## Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q7_code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- SVM Kernel Comparison (Wine Dataset) ---\n",
            "Accuracy (Linear Kernel): 0.9815\n",
            "Accuracy (RBF Kernel): 0.9815\n",
            "\n",
            "Observation: Both kernels performed equally well.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load and prepare data\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Scale features (important for SVM, especially RBF kernel)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train Linear Kernel SVM\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "acc_linear = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# 3. Train RBF Kernel (Non-linear) SVM\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "acc_rbf = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "# 4. Compare Accuracies\n",
        "print(\"--- SVM Kernel Comparison (Wine Dataset) ---\")\n",
        "print(f\"Accuracy (Linear Kernel): {acc_linear:.4f}\")\n",
        "print(f\"Accuracy (RBF Kernel): {acc_rbf:.4f}\")\n",
        "\n",
        "if acc_rbf > acc_linear:\n",
        "    print(\"\\nObservation: The RBF (non-linear) kernel provided a slightly better accuracy, suggesting the data is not perfectly linearly separable.\")\n",
        "elif acc_linear > acc_rbf:\n",
        "    print(\"\\nObservation: The Linear kernel performed better, suggesting the data is either linear or the RBF hyper-parameters need tuning.\")\n",
        "else:\n",
        "    print(\"\\nObservation: Both kernels performed equally well.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8"
      },
      "source": [
        "## Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8"
      },
      "source": [
        "The **Naïve Bayes classifier** is a family of probabilistic classification algorithms based on **Bayes' Theorem**.\n",
        "\n",
        "* **Bayes' Theorem:** It calculates the probability of a hypothesis (a data point belonging to a certain class) given the evidence (the features of the data point).\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "* **Why it's \"Naïve\":** The classifier is called *Naïve* because it makes the fundamental, simplifying, and often unrealistic assumption of **conditional independence** among the features. \n",
        "\n",
        "    * In simple terms, it assumes that the presence of a specific feature in a class is unrelated to the presence of any other feature. For example, in a text classification problem, Naïve Bayes assumes that the probability of the word \"yellow\" appearing is independent of the word \"taxi\" appearing, given the category (e.g., transportation). Despite this strong and sometimes false assumption, Naïve Bayes often performs surprisingly well in practice, especially in text processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9"
      },
      "source": [
        "## Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9"
      },
      "source": [
        "The three primary variants of Naïve Bayes classifiers differ based on the type of data they are designed to model and the underlying statistical distribution they assume for the features ($P(B|A)$ in Bayes' Theorem).\n",
        "\n",
        "| Variant | Feature Type | Typical Application |\n",
        "| :--- | :--- | :--- |\n",
        "| **Gaussian Naïve Bayes** | **Continuous** (e.g., Height, Weight, Sensor Readings) | Assumes that continuous features associated with each class are distributed according to a **Gaussian (Normal) distribution**. |\n",
        "| **Multinomial Naïve Bayes** | **Count Data/Discrete** (e.g., Word counts in a document) | Used when features represent the frequency of events (counts), most commonly for **text classification** where features are word counts or frequencies. |\n",
        "| **Bernoulli Naïve Bayes** | **Binary/Boolean** (e.g., Feature present or absent, 0 or 1) | Used when features are binary. In text classification, it checks for the **presence or absence** of a word in a document, rather than its frequency. |\n",
        "\n",
        "The key is matching the model type to the data's distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q10"
      },
      "source": [
        "## Question 10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q10_code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Gaussian Naïve Bayes (Breast Cancer Dataset) ---\n",
            "Model Accuracy: 0.9415\n",
            "\n",
            "Interpretation: A high accuracy indicates that the Naïve Bayes model, despite its independence assumption, is effective at classifying benign vs. malignant tumors based on the continuous feature measurements (like mean radius, texture, etc.) in this dataset.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize and train the Gaussian Naïve Bayes classifier\n",
        "# GaussianNB is used because the features are continuous measurements\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions and evaluate accuracy\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"--- Gaussian Naïve Bayes (Breast Cancer Dataset) ---\")\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nInterpretation: A high accuracy indicates that the Naïve Bayes model, despite its independence assumption, is effective at classifying benign vs. malignant tumors based on the continuous feature measurements (like mean radius, texture, etc.) in this dataset.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Supervised Classification Solution",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
